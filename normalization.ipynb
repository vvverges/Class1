{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"Sentences and nominals share several properties. Verbs and nouns are semantically central elements of their phrases.\n",
    "        He is cooking. better or worse. loved, loving, loves, love \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentenc', 'and', 'nomin', 'share', 'sever', 'properti', '.', 'verb', 'and', 'noun', 'are', 'semant', 'central', 'element', 'of', 'their', 'phrase', '.', 'He', 'is', 'cook', '.', 'better', 'or', 'wors', '.', 'love', ',', 'love', ',', 'love', ',', 'love']\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "porter = nltk.PorterStemmer()\n",
    "print([porter.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming :\n",
    "원래는 derives, distributing등 단어가 원형에서 활용한 형태였는데, 뒷부분을 잘라 원형에 가깝게 나누어준다.\n",
    "단순히 어근으로 추정되는 부분만 추출하기 때문에, dennis->denni,women,derives 등 잘못 잘리는 경우도 존재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentences', 'and', 'nominal', 'share', 'several', 'property', '.', 'Verbs', 'and', 'noun', 'are', 'semantically', 'central', 'element', 'of', 'their', 'phrase', '.', 'He', 'is', 'cooking', '.', 'better', 'or', 'worse', '.', 'loved', ',', 'loving', ',', 'love', ',', 'love']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming과 lemmatize의 차이\n",
    "stemming : 단어의 어근(핵심)이 되는 부분과 그렇지 않은 부분을 구분하여 affix등을 떼어낸다. (lemmatize보다 훨씬 일률적인(?)작업)\n",
    "lemmatize : 단어의 어근뿐만 아니라 형태(품사)상태를 구분하여 온전한 형태로 단어를 나눈다. \n",
    "(예를들어, stemming에서는 semantically가 semant, loving이 love지만, lemmatize에서는 각각 semantically, loving등 품사 형태를 유지하면서 도출되었다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
